# Hear Me Move

## Introduction
	Using PoseNet and Wekinator, Hear Me Move transforms movement generation into a way of concurrently generating audio, blurring the distinction of influence between these two art forms. Historically, dance has been used as a way to physically interpret and embody music and sound. But what happens when we reverse the dynamic between movement and sound? How can a machine-interpretation of our own movement both surprise and challenge us? Hear Me Move gives users a real-time translation of their movement into sound, confronting users with a reflection of their own physicality. What possibilities will this confrontation open up in terms of user response?
    
    As dancers, it was a natural choice for us to explore how we can expand the limits of our movement through technology. Used in combination with dance improvisation, we aim to explore our own responses to this process of co-creation with our machine-generated audio. How will the implementation of this audio-generating technology influence our movement? This approach opens up many creative doors beyond movement itself: we can prioritize the soundscape instead of our movement, prioritize movement instead of the soundscape, dance in response to the soundscape, or some combination of all the above. In this process, greater significance is placed on the dancer’s intent when performing. Given the unpredictability of the machine-generated audio, Hear Me Move will refresh dancers’ improvisation practices and allow for novel explorations of movement.


## Framework and Process
    Hear Me Move links PoseNet to Wekinator to generate audio from real-time pose-estimation. For our input, we used PoseNet to detect movement through the computer webcam and adapted part of Maya Man’s Google Creative Lab PoseNet Sketchbook to render the pose skeleton onto our webcam canvas. For our output, we augmented the Continuously-controlled drum machine from Wekinator’s example code to include more outputs, using samples from Genius Home Studio’s beat pack. 

    With our inputs and outputs set up, we each trained Wekinator on several poses linked to audio of our choosing. Once the training was finished, we ran Wekinator and recorded ourselves improvising to the generated audio. 


## How Our Vision Shifted
	Our original goal was to explore how we can use technology and ML to stretch the limits of our physical body and give our bodily agency to a computer. Intrigued by the impossibility of some choreography phrases generated by Wayne McGregor’s Living Archive, we aspired to map this machine-generated choreography onto our own bodies. 
    
    We would detect the choreography poses with Maya Man’s Google Creative Lab PoseNet Sketchbooks and feed the key points to Caroline Chan’s Everybody Dance Now in order to translate the choreography to our bodies. By juxtaposing the video of the mapped choreography against a video where we perform the choreography, we hoped to expose the ways in which technology can increase our physical capabilities.

	We ran into some difficulties in using these frameworks, which ultimately changed our project direction. We tried using the PoseNet Sketchbooks to detect the poses from the choreography phrase, but since Living Archive generates choreography using stick figures, Posenet Sketchbooks was unable to recognize the stick figure as a pose. The code for Living Archive is not public, so we would have to manually extract key points of every frame and write our own function to interpolate them before we could feed the data to Everybody Dance Now. With so many challenges in our frameworks, we shifted to using Wekinator instead. 


## Problems + How We Solved
	The first problem we encountered in using Wekinator was getting PoseNet to use webcam data as input. We were running PoseNet on Processing which uses GStreamer to access the webcam, but GStreamer doesn’t work for OSX 10.14+. Instead, we found a different way to extract webcam data with PoseNet using JavaScript. 
    
    Though it was relatively simple to run PoseNet with webcam data from the browser, we encountered our biggest challenge with figuring out how to send data from the browser into Wekinator. JavaScript doesn’t let developers send UDP packets, which Wekinator receives as input, so we had to figure out how to circumvent this issue. We ended up referencing multiple repositories in order to see how other people fed input into Wekinator. We found that we had to wrap the entire program in an Express app so that we could send the pose data as UDP packets.
    
    Once we knew the data was being successfully communicated to Wekinator, we tried to draw the pose prediction skeleton onto the webcam canvas. It was difficult to understand how to properly use the coordinates from PoseNet, so we ended up using one of the JavaScript files for drawing pose skeletons from Maya Man’s PoseNet Sketchbooks.
    
    Another issue we ran into was tuning our PoseNet predictions so they would accurately estimate our poses. Originally, we displayed all pose predictions and had difficulties figuring out how to filter out low-quality predictions. Frames would jump between positions on the webcam canvas, and the input recognition was not very precise when we tried training in Wekinator. To solve this, we use a moving average of the predictions with an eviction policy to ensure data does not stay stuck on the buffer.

## Description of Final Product
